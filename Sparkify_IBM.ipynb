{"cells": [{"metadata": {}, "cell_type": "code", "source": "import ibmos2spark\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-843533b3-31d4-49f3-aec1-18fb437ef9c2',\n    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n    'api_key': 'hZ_NjVVnMgNQvLCH-U9qE3THdy8oG-bgP0ELmbYThz6Z'\n}\n\nconfiguration_name = 'os_3362a26b7d834f86be1984c8cd9e69e9_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "in_schema = \"artist STRING, auth STRING, firstName STRING, gender STRING, itemInSession INT, lastName STRING,\" \\\n            \"length DOUBLE, level STRING, location STRING, method STRING, page STRING, registration LONG,\" \\\n            \"sessionId INT, song STRING, status int, ts LONG, userAgent STRING, userId STRING\"\n\ndf_raw = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkify-donotdelete-pr-wxvl4d9mrqe9ee'), schema=in_schema)\\\n    .withColumn('iuid', F.col('userId').cast('long')).drop('userId').withColumnRenamed('iuid', 'userId')\n\ndf_raw.take(5)", "execution_count": 9, "outputs": [{"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "[Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId=293),\n Row(artist=\"John Brown's Body\", auth='Logged In', firstName='Sawyer', gender='M', itemInSession=74, lastName='Larson', length=380.21179, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='PUT', page='NextSong', registration=1538069638000, sessionId=97, song='Bulls', status=200, ts=1538352025000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=98),\n Row(artist='Afroman', auth='Logged In', firstName='Maverick', gender='M', itemInSession=184, lastName='Santiago', length=202.37016, level='paid', location='Orlando-Kissimmee-Sanford, FL', method='PUT', page='NextSong', registration=1535953455000, sessionId=178, song='Because I Got High', status=200, ts=1538352118000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=179),\n Row(artist=None, auth='Logged In', firstName='Maverick', gender='M', itemInSession=185, lastName='Santiago', length=None, level='paid', location='Orlando-Kissimmee-Sanford, FL', method='PUT', page='Logout', registration=1535953455000, sessionId=178, song=None, status=307, ts=1538352119000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=179),\n Row(artist='Lily Allen', auth='Logged In', firstName='Gianna', gender='F', itemInSession=22, lastName='Campos', length=194.53342, level='paid', location='Mobile, AL', method='PUT', page='NextSong', registration=1535931018000, sessionId=245, song='Smile (Radio Edit)', status=200, ts=1538352124000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId=246)]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df_explore = df_raw\ndf_explore.head()", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId=293)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import sys\n!{sys.executable} -m pip install plotly", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "Collecting plotly\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/5f/47ab0d9d843c5be0f5c5bd891736a4c84fa45c3b0a0ddb6b6df7c098c66f/plotly-4.9.0-py2.py3-none-any.whl (12.9MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.9MB 987kB/s eta 0:00:01\n\u001b[?25hCollecting retrying>=1.3.3 (from plotly)\n  Downloading https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz\nCollecting six (from plotly)\n  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nBuilding wheels for collected packages: retrying\n  Building wheel for retrying (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c\nSuccessfully built retrying\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n\u001b[31mpytest-doctestplus 0.7.0 has requirement pytest>=4.0, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\nInstalling collected packages: six, retrying, plotly\nSuccessfully installed plotly-4.9.0 retrying-1.3.3 six-1.15.0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import plotly.express as px\nimport plotly.graph_objects as go", "execution_count": 36, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_cleaned = df_raw.dropna(how='any', subset=['userId', ])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# model.py\ninstead of uploading model.py it is easier here to just paste the contents"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import functions as F\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Transformer\n\n\nclass UserLogTransformer(Transformer):\n    \"\"\"\n    Custom transformer that aggregates Spotify log data into user features for predicting\n    if they are at risk of churning.\n\n    Extracted user information are:\n\n    * user base data:\n        * gender\n        * level\n        * registration (time tamp)\n    * aggregated log data\n        * avg_session_events  mean number of log events per session\n        * min_session_events\n        * max_session_events\n        * total_session_events\n        * ts_min  timestamp of earliest log event\n        * ts_max  timestamp of most recent log event\n        * period  interval in days between earliest and latest log event\n        * counts of log event types (pages)\n            * About\n            * Add Friend\n            * Add to Playlist\n            * Error\n            * Help\n            * Home\n            * Logout\n            * NextSong\n            * Roll Advert\n            * Save Settings\n            * Settings\n            * Thumbs Down\n            * Thumbs Up\n            * Upgrade\n            * Submit Downgrade\n            * Submit Upgrade\n        * frequencies of log event types defined as count / period for the events above\n        * 1-hot encoded list of operating systems seen on userId\n        * 1-hot encoded list of browser engines seen on userId\n\n    Events interpreted as churn are deleted from the dataset:\n    * Cancellation Confirmation\n\n    Churn-related events that are deleted to avoid data leakage:\n    * Cancel\n    \"\"\"\n\n    def _transform(self, dataset):\n        # splitting up operations into a few temporary data tables for readability\n        # session statistics\n        df_session_stats = dataset.select('userId', 'sessionId', 'itemInSession').groupBy('userId', 'sessionId') \\\n            .count().fillna(0, 'count').withColumnRenamed(\"count\", \"_count\") \\\n            .groupBy('userId').agg(F.mean(\"_count\").cast('double').alias(\"avg_session_events\"),\n                                   F.min(\"_count\").cast('double').alias(\"min_session_events\"),\n                                   F.max(\"_count\").cast('double').alias(\"max_session_events\"),\n                                   F.sum('_count').cast('double').alias(\"total_session_events\"))\n        # user interaction period stats and per user stats\n        df_user_categories = dataset.select('userId', F.when(dataset.gender == 'f', 1.0).otherwise(0.0).alias('_gender'),\n                                            F.when(dataset.level == 'paid', 1.0).otherwise(0.0).alias('_level'))\\\n            .groupBy('userId').agg(F.first('_gender').alias('gender'), F.max('_level').alias('maxLevel'),\n                                   (F.max('_level') - F.min('_level')).alias('changedLevel'))\n        df_by_user = dataset.select('userId', 'ts', 'registration').groupBy('userId')\\\n            .agg(#F.min('ts').cast('double').alias(\"ts_min\"),\n                 #F.max('ts').cast('double').alias(\"ts_max\"),\n                 F.first('registration').cast('double').alias('registration'),\n                 ((F.max('ts')-F.min('ts')) / (3600 * 24 * 1000)).alias('period')\n                 )\n        # user browser and operating system counts\n        df_os_browser = dataset.select('userId', 'userAgent',\n                                       F.regexp_replace(F.regexp_extract(F.col('userAgent'), \".*?(\\(.*\\)).*\", 1),\n                                        '[\\(\\);:;\\s\\/.,]+', '').alias('os'),\n                                       F.regexp_replace(F.regexp_extract(F.col('userAgent'), \".*\\s(.*)\", 1),\n                                        '[\\(\\);:;\\s\\/.,]+', '').alias('browser'))\n        df_os_onehot = df_os_browser.groupBy('userId').pivot('os')\\\n            .agg(F.countDistinct('userId').cast('double').alias('os')).fillna(0)\n        df_browser_onehot = df_os_browser.groupBy('userId').pivot('browser') \\\n            .agg(F.countDistinct('userId').cast('double').alias('browser')).fillna(0)\n        # user page counts and frequencies\n        df_page_counts = dataset.select('userId', F.column('page').alias('page'))\\\n            .join(df_by_user.select('userId', 'period'), on='userId')\\\n            .groupBy('userId').pivot('page').agg(F.count('userId').cast('double').alias('count'),\n                                                 (F.count('userId') / F.first('period')).cast('double')\n                                                 .alias('freq')).fillna(0)\\\n            .drop('Cancel_count', 'Cancellation Confirmation_count', 'Submit Downgrade_count',\n                  'Cancel_freq', 'Cancellation Confirmation_freq', 'Submit Downgrade_freq')\n        return df_by_user.join(df_page_counts, on='userId')\\\n            .join(df_user_categories, on='userId')\\\n            .join(df_os_onehot, on='userId')\\\n            .join(df_browser_onehot, on='userId')\\\n            .join(df_session_stats, on='userId')\n\n\nclass LogCleanTransformer(Transformer):\n    \"\"\"\n    Custom Transformer that cleans user log data for machine learning.\n\n    Drops all rows with nan userId values\n    \"\"\"\n\n    def _transform(self, dataset):\n        return dataset.dropna(how='any', subset=['userId', ])\n\n\nclass UserLabelTransformer(Transformer):\n    \"\"\"\n    Custom Transformer that returns if a user has churn events in his log history\n\n    Defined Churn events are:\n    * Cancellation Confirmation\n    * Submit Downgrade\n    \"\"\"\n\n    def _transform(self, dataset):\n        return dataset.select('userId', F.when((dataset.page == 'Cancellation Confirmation'),\n                                               1).otherwise(0).alias('churn'))\\\n            .groupBy('userId').agg(F.max('churn').alias('churned'))\n\n\nclass TrainingAssembler(Transformer):\n    \"\"\"\"\n    Assemble the data set for training into label and feature vector columns.\n\n    Expects a dataframe of numeric columns, one of which should be named 'churned'. The 'churned' column is used as the\n    labels columns. All other columns are assembled into the features vector.\n    \"\"\"\n\n    def _transform(self, dataset):\n        input_cols = dataset.columns\n        input_cols.remove('churned')\n        vectassemble = VectorAssembler(inputCols=input_cols, outputCol='features', handleInvalid='skip')\n        return vectassemble.transform(dataset).select(\n            F.column('churned').alias('label'), 'features', 'userId'\n        )\n\n\nclass MasterTransformer(Transformer):\n    \"\"\"\n    Transformer instance that puts together the building blocks for label and feature extraction\n    \"\"\"\n\n    def _transform(self, dataset):\n        logtransform = UserLogTransformer()\n        labeltransform = UserLabelTransformer()\n        assembler = TrainingAssembler()\n        return assembler.transform(logtransform.transform(dataset).join(labeltransform.transform(dataset), on='userId'))\n", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Modeling\nHere we use the insights gained fromt he exploration ont he small sample of the full data set. Therefore, here the data is only split up inti a training and test set. Validation will be skipped, since we will beusing the classification method and hyperparameters obtained there. I.e. we use a linear support vector machine with maximum iterations set to 300 and and a regularization parameter of 1E-4.\n\n## Preparation\nImport required classes, transform log data to features and split off test data. Will use 10% of userIds as test dataset. The remainder will be used for training.\n"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import LinearSVC", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "m_tra = MasterTransformer()\ncleaner = LogCleanTransformer()\nmt = m_tra.transform(cleaner.transform(df_raw))", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "mt.head(30)\ntrain, test = mt.randomSplit([0.90, 0.10], seed=1404)", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "lsvc = LinearSVC(maxIter=300, regParam=1e-4)\nmodel_lsvc = lsvc.fit(train)\n", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pred_lsvc = model_lsvc.transform(test).withColumn('tp', F.col('label') * F.col('prediction'))\\\n    .withColumn('fp', (1 - F.col('label')) * F.col('prediction'))\\\n    .withColumn('fn', F.col('label') * (1 - F.col('prediction')))", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "tp_sum = pred_lsvc.agg(F.sum('tp').alias('tp_s')).collect()[0]['tp_s']\nfp_sum = pred_lsvc.agg(F.sum('fp').alias('fp_s')).collect()[0]['fp_s']\nfn_sum = pred_lsvc.agg(F.sum('fn').alias('fn_s')).collect()[0]['fn_s']\nF1_rf = tp_sum / (tp_sum + 0.5 * (fp_sum + fn_sum))\nprint(\"Linear SVC regression F1 score :\", F1_rf)", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Linear SVC regression F1 score : 0.9230769230769231\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(model_lsvc.explainParams())", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\nfeaturesCol: features column name (default: features)\nfitIntercept: whether to fit an intercept term (default: True)\nlabelCol: label column name (default: label)\nmaxIter: maximum number of iterations (>= 0) (default: 100, current: 300)\npredictionCol: prediction column name (default: prediction)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\nregParam: regularization parameter (>= 0) (default: 0.0, current: 0.0001)\nstandardization: whether to standardize the training features before fitting the model (default: True)\nthreshold: threshold in binary classification prediction applied to rawPrediction (default: 0.0)\ntol: the convergence tolerance for iterative algorithms (>= 0) (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "model_lsvc.write().overwrite().save(cos.url('lsvc_model', 'sparkify-donotdelete-pr-wxvl4d9mrqe9ee'))", "execution_count": 37, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pred_lsvc.coalesce(1).write.mode(\"overwrite\").json(cos.url('lsvc-prediction.json', 'sparkify-donotdelete-pr-wxvl4d9mrqe9ee'))", "execution_count": 39, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "test.coalesce(1).write.json(cos.url('testdata.json', 'sparkify-donotdelete-pr-wxvl4d9mrqe9ee'))", "execution_count": 40, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train.coalesce(1).write.json(cos.url('traindata.json', 'sparkify-donotdelete-pr-wxvl4d9mrqe9ee'))", "execution_count": 41, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}